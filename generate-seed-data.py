#!/usr/bin/env python3
"""
generate-seed-data.py — Process spot history CSV into seed data for StakTrakr.

Usage:
    python3 generate-seed-data.py <csv-file> [--dry-run]

Reads a StakTrakr spot history CSV export and produces:
  1. data/spot-history-YYYY.json  — one file per year (flat JSON arrays)
  2. js/seed-data.js              — loader module with embedded fallback data

Only "api" source entries are kept. Entries are deduplicated to one per
metal per calendar day (first occurrence wins), timestamps are normalised
to noon, and all entries are tagged source:"seed", provider:"StakTrakr".
"""

import csv
import json
import os
import sys
from collections import OrderedDict
from datetime import datetime

REPO_ROOT = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(REPO_ROOT, "data")
SEED_JS_PATH = os.path.join(REPO_ROOT, "js", "seed-data.js")

VALID_METALS = {"Silver", "Gold", "Platinum", "Palladium"}


def parse_csv(csv_path):
    """Parse CSV and return list of dicts with Timestamp, Metal, Price, Source, Provider."""
    rows = []
    with open(csv_path, newline="", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            rows.append(row)
    return rows


def process_rows(rows):
    """Filter, deduplicate, normalise and tag rows. Returns sorted list of seed entries."""
    # Step 1: Filter to api source only
    api_rows = [r for r in rows if r.get("Source", "").strip().lower() == "api"]

    # Step 2: Deduplicate — one entry per metal per calendar day (first wins)
    seen = set()
    deduped = []
    for row in api_rows:
        metal = row.get("Metal", "").strip()
        if metal not in VALID_METALS:
            continue
        price_str = row.get("Price", "").strip()
        try:
            price = round(float(price_str), 4)
        except (ValueError, TypeError):
            continue
        if price <= 0:
            continue

        ts_str = row.get("Timestamp", "").strip()
        try:
            dt = datetime.strptime(ts_str[:10], "%Y-%m-%d")
        except (ValueError, IndexError):
            continue

        day_key = (metal, dt.strftime("%Y-%m-%d"))
        if day_key in seen:
            continue
        seen.add(day_key)

        # Step 3: Normalise timestamp to noon
        normalised_ts = dt.strftime("%Y-%m-%d") + " 12:00:00"

        deduped.append({
            "spot": price,
            "metal": metal,
            "source": "seed",
            "provider": "StakTrakr",
            "timestamp": normalised_ts,
        })

    # Step 4: Sort chronologically
    deduped.sort(key=lambda e: e["timestamp"])
    return deduped


def split_by_year(entries):
    """Split entries into {year: [entries]} ordered dict."""
    by_year = OrderedDict()
    for entry in entries:
        year = int(entry["timestamp"][:4])
        by_year.setdefault(year, []).append(entry)
    return by_year


def write_json_files(by_year, dry_run=False):
    """Write data/spot-history-YYYY.json for each year."""
    if not dry_run:
        os.makedirs(DATA_DIR, exist_ok=True)

    for year, entries in by_year.items():
        path = os.path.join(DATA_DIR, f"spot-history-{year}.json")
        content = json.dumps(entries, indent=2)
        if dry_run:
            print(f"  [DRY RUN] Would write {path} ({len(entries)} entries, {len(content)} bytes)")
        else:
            with open(path, "w", encoding="utf-8") as f:
                f.write(content + "\n")
            print(f"  Wrote {path} ({len(entries)} entries, {len(content)} bytes)")


def generate_seed_js(by_year, all_entries, dry_run=False):
    """Generate js/seed-data.js with loader, config, and embedded fallback."""
    years_list = sorted(by_year.keys())
    years_js = json.dumps(years_list)

    # Build compressed embedded data (one JSON line per entry, no extra whitespace)
    embedded_lines = []
    for entry in all_entries:
        embedded_lines.append(json.dumps(entry, separators=(",", ":")))
    embedded_json = "[\n  " + ",\n  ".join(embedded_lines) + "\n]"

    js_content = f'''\
// SEED SPOT PRICE HISTORY — Auto-generated by generate-seed-data.py
// Provides historical spot prices for first-time users so sparklines
// and price cards are populated on first load.
// =============================================================================

/**
 * Years with available seed data. Updated by generate-seed-data.py.
 * @constant {{number[]}}
 */
const SEED_DATA_YEARS = {years_js};

/**
 * Embedded seed data fallback for file:// protocol where fetch() cannot
 * load local JSON files. Auto-generated — do not edit by hand.
 *
 * @returns {{Array<{{spot:number, metal:string, source:string, provider:string, timestamp:string}}>}}
 */
function getEmbeddedSeedData() {{
  return {embedded_json};
}}

/**
 * Loads seed spot history for first-time users.
 *
 * Guard: skips if spotHistory already has entries (existing user).
 * Tries fetch() for each year's JSON file; falls back to embedded data
 * for file:// protocol. Validates entries, bulk-assigns to spotHistory,
 * and persists with a single saveSpotHistory() call.
 *
 * Also sets the latest seed price per metal in localStorage so price
 * cards display seed prices instead of hardcoded defaults.
 *
 * @returns {{Promise<void>}}
 */
async function loadSeedSpotHistory() {{
  // Guard: existing users already have history
  if (typeof spotHistory !== "undefined" && spotHistory.length > 0) {{
    debugLog("Seed data: skipped — spotHistory already has " + spotHistory.length + " entries");
    return;
  }}

  debugLog("Seed data: loading for first-time user...");

  let seedEntries = [];

  // Try fetching JSON files (works on HTTP, fails on file://)
  try {{
    const fetches = SEED_DATA_YEARS.map(async (year) => {{
      const resp = await fetch("data/spot-history-" + year + ".json");
      if (!resp.ok) throw new Error("HTTP " + resp.status);
      return resp.json();
    }});
    const results = await Promise.all(fetches);
    results.forEach((entries) => {{
      if (Array.isArray(entries)) seedEntries = seedEntries.concat(entries);
    }});
  }} catch (e) {{
    debugLog("Seed data: fetch failed (" + e.message + "), using embedded fallback");
    seedEntries = getEmbeddedSeedData();
  }}

  // Validate entries
  seedEntries = seedEntries.filter(
    (e) => e && typeof e.spot === "number" && e.spot > 0 && e.metal && e.timestamp
  );

  if (seedEntries.length === 0) {{
    debugLog("Seed data: no valid entries found");
    return;
  }}

  // Sort chronologically
  seedEntries.sort((a, b) => a.timestamp.localeCompare(b.timestamp));

  // Bulk assign and persist
  spotHistory = seedEntries;
  if (typeof saveSpotHistory === "function") {{
    saveSpotHistory();
  }}

  // Set latest seed price per metal in localStorage so fetchSpotPrice()
  // picks them up instead of hardcoded defaults
  const latestByMetal = {{}};
  for (const entry of seedEntries) {{
    latestByMetal[entry.metal] = entry.spot;
  }}

  if (typeof METALS !== "undefined") {{
    Object.values(METALS).forEach((mc) => {{
      const price = latestByMetal[mc.name];
      if (price && price > 0) {{
        localStorage.setItem(mc.localStorageKey, String(price));
      }}
    }});
  }}

  debugLog("Seed data: loaded " + seedEntries.length + " entries (" + Object.keys(latestByMetal).length + " metals)");
}}

// Export for global access
window.loadSeedSpotHistory = loadSeedSpotHistory;
'''

    if dry_run:
        print(f"  [DRY RUN] Would write {SEED_JS_PATH} ({len(js_content)} bytes)")
        print(f"  [DRY RUN] SEED_DATA_YEARS = {years_js}")
        print(f"  [DRY RUN] Embedded entries: {len(all_entries)}")
    else:
        with open(SEED_JS_PATH, "w", encoding="utf-8") as f:
            f.write(js_content)
        print(f"  Wrote {SEED_JS_PATH} ({len(js_content)} bytes)")


def main():
    if len(sys.argv) < 2:
        print(__doc__)
        sys.exit(1)

    csv_path = sys.argv[1]
    dry_run = "--dry-run" in sys.argv

    if not os.path.isfile(csv_path):
        print(f"Error: file not found: {csv_path}")
        sys.exit(1)

    print(f"Processing: {csv_path}")
    if dry_run:
        print("Mode: DRY RUN (no files written)\n")
    else:
        print()

    rows = parse_csv(csv_path)
    print(f"  CSV rows read: {len(rows)}")

    entries = process_rows(rows)
    print(f"  After filter+dedup: {len(entries)} entries")

    # Stats per metal
    metal_counts = {}
    for e in entries:
        metal_counts[e["metal"]] = metal_counts.get(e["metal"], 0) + 1
    for metal, count in sorted(metal_counts.items()):
        print(f"    {metal}: {count} days")

    by_year = split_by_year(entries)
    print(f"\n  Years: {sorted(by_year.keys())}")
    for year, year_entries in by_year.items():
        print(f"    {year}: {len(year_entries)} entries")

    print("\nWriting JSON files:")
    write_json_files(by_year, dry_run)

    print("\nGenerating js/seed-data.js:")
    generate_seed_js(by_year, entries, dry_run)

    print("\nDone!")


if __name__ == "__main__":
    main()
